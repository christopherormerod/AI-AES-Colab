{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgWbrRcWhkGdKd53yEg+w6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christopherormerod/AI-AES-Colab/blob/main/FullExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_id = \"llm-aes/asap-7-original\"\n",
        "data = datasets.load_dataset(data_id)\n",
        "df = pd.DataFrame(data[\"train\"])\n",
        "train, test = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NojJiMKqdZci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aes_metrics(y1, y2):\n",
        "  qwk = sklearn.metrics.cohen_kappa_score(y1, y2, weights=\"quadratic\")\n",
        "  acc = sklearn.metrics.accuracy_score(y1, y2)\n",
        "  smd_numerator = np.mean(y1) - np.mean(y2)\n",
        "  smd_denominator = np.sqrt((np.std(y1)**2 + np.std(y2)**2)/2)\n",
        "  smd = smd_numerator / smd_denominator\n",
        "  return {\"QWK\": qwk, \"Acc\": acc, \"SMD\":smd}"
      ],
      "metadata": {
        "id": "d7a60KRldXcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUmWDvkhdQfq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "class FineTunedEssayScorer:\n",
        "\n",
        "  def __init__(self, model_id, max_score, min_score):\n",
        "    self.max_score = max_score\n",
        "    self.min_score = min_score\n",
        "    self.classifier = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels = max_score - min_score + 1)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    if torch.cuda.is_available():\n",
        "      self.classifier.cuda()\n",
        "\n",
        "  def train(self, X, y, epochs = 10):\n",
        "    self.classifier.train()\n",
        "    optimizer = torch.optim.AdamW(self.classifier.parameters(), lr=5e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=epochs)\n",
        "    best_state = self.classifier.state_dict()\n",
        "    best_score = -1\n",
        "    N = len(X)\n",
        "    for e in range(epochs):\n",
        "      for i in tqdm(range(N)):\n",
        "        optimizer.zero_grad()\n",
        "        X_batch = self.tokenizer(X[i],\n",
        "                                 return_tensors='pt',\n",
        "                                 padding=\"max_length\",\n",
        "                                 truncation=True,\n",
        "                                 max_length=512).to(self.classifier.device)\n",
        "        y_batch = y[i] - self.min_score\n",
        "        outputs = self.classifier(**X_batch, labels=torch.tensor([y_batch]).to(self.classifier.device))\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sheduler.step()\n",
        "      metrics = self.evalate(X, y)\n",
        "      if metrics[\"QWK\"] > best_score:\n",
        "        self.best_state = self.classifier.state_dict()\n",
        "        best_score = metrics[\"QWK\"]\n",
        "    self.classifier.load_state_dict(self.best_state)\n",
        "\n",
        "  def evalate(self, X, y):\n",
        "    self.classifier.eval()\n",
        "    scores = self.score(X)\n",
        "    return aes_metrics(y, scores)\n",
        "\n",
        "  def score(self, X):\n",
        "    self.classifier.eval()\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "      for X_batch in tqdm(X):\n",
        "        X_batch = self.tokenizer(X_batch,\n",
        "                                 return_tensors='pt',\n",
        "                                 padding=\"max_length\",\n",
        "                                 truncation=True,\n",
        "                                 max_length=512).to(self.classifier.device)\n",
        "        outputs = self.classifier(**X_batch)\n",
        "        scores.append(int(outputs.logits.cpu().argmax(dim=1)) + self.min_score)\n",
        "    return scores"
      ]
    }
  ]
}